# -*- coding: utf-8 -*-
"""combined_10/26.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-e1gPJ3WJ2zhdQfxHfvzZbnknzwcSNvq
"""

import streamlit as st

st.write("My first app")

#Importing the libraries to be used
#import os
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import itertools
import collections
import re

#import panel as pn
#pn.extension('tabulator')
#!pip install hvplot
#import hvplot.pandas
!pip install dataframe_image
import dataframe_image as dfi

import tweepy as tw
from textblob import TextBlob
#!pip install pygal
#import pygal
!pip install vaderSentiment
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import TweetTokenizer 
from nltk import bigrams
from textblob import TextBlob
import networkx # for creating networknodes
import networkx as nx
#from pandas.io.json import json_normalize

import warnings
warnings.filterwarnings("ignore")

sns.set(font_scale=1.5)
sns.set_style("whitegrid")

#import configparser
#!pip install tweepy
import tweepy
import pandas as pd

api_key = "CWW5rvfohMPrJaf4KuACiE0FN"
api_key_secret = "r6iRyGGXzY9iwQaxjAjA64WweBIUFZnPM6tVpVfUMIrx3zlGtq"

access_token = "3403682451-rwoiYIUkNtEeNDZ97ZoRzbI5daYm9vOKRnzXlan"
access_token_secret = "QdrHp0YVqfIdVpyZVhTgIUSnGsOnL24sWEQUqUdbhUxVK"

#authentication
auth = tweepy.OAuthHandler(api_key, api_key_secret)
auth.set_access_token(access_token, access_token_secret)

api = tweepy.API(auth)

keywords = input("Enter the keyword to get tweets :: ")
keywords = keywords #Specify keywords to search for
limit = 1000 #Number of tweets to obtain


#limit at a time , we get 200.To solve this issue run code below.
tweets = tweepy.Cursor(api.search, q=keywords, count=200, tweet_mode = 'extended', wait_on_rate_limit=True).items(limit)

#Create DataFrame
columns = ['created_at', 'text', 'User']#, 'reply_count',	'retweet_count']#, 'user/location', 'user/followers_count']
data = []

for tweet in tweets:
  data.append([ tweet.created_at, tweet.full_text, tweet.user.screen_name])#, tweet.reply_count, tweet.retweet_count])# tweet.user.location, tweet.user.followers_count])

df = pd.DataFrame(data, columns=columns)
#df.rename(columns={"full_text": "text"}, inplace=True)

df.head(5)

#Checking the info of data
df.info()

#Removing the duplicates
df = df.drop_duplicates()
df.info()

#Convert the created_at column to datetime datatype
df['created_at'] = df['created_at'].astype('datetime64[ns]')
df.info()



"""## Checking the Date ranges and the peak hours"""

#Creating a column for hour
df['hour'] = df['created_at'].dt.hour
#Creating a column for days
df['date'] = df['created_at'].dt.date
#Creating a column for month
df['month'] = df['created_at'].dt.month
df.head()

#Checking the unique dates
df['date'].value_counts()#Checking the unique dates

# time series showing when the tweets for this analysis was created
reactions = df.groupby(['date']).count()
ax = reactions.text.plot(figsize=(15,6),ls='--',c='blue')
plt.ylabel('The Count of tweets collected')
plt.title('A Trend on the counts of tweets and the dates created' , fontsize=15, color= 'brown', fontweight='bold')
ax.xaxis.grid(True)
ax.yaxis.grid(True)

reactions = df.groupby(['hour']).count().sort_values(by='created_at',ascending=0)
reactions.head()

# time series plot for the most active hours for tweeting
reactions = df.groupby(['hour']).count()
ax = reactions.text.plot(figsize=(15,6),ls='--',c='green')
plt.ylabel('The Count of tweets collected')
plt.title('A Trend on the counts of tweets and the hours created',  fontsize=15, color= 'green', fontweight='bold')
ax.xaxis.grid(True)
ax.yaxis.grid(True)



"""## Exploratory Data Analysis (EDA)"""

#Creating a copy for the text column This will enable us work with the text column solely
df_tweets = df[['text']].copy()
df_tweets.tail(5)

#Dropping the duplicates
df_tweets = df_tweets.drop_duplicates()

df_tweets.tail(5)



"""## Text Processing"""

#A Function for cleaning the file (The text column in it)
def text_clean(df_tweets):
  #Lowercasing all the letters
  df_tweets['text'] = df_tweets['text'].str.lower() 

  #Removes mentions containing rt word
  df_tweets['text'] = df_tweets['text'].str.replace(r'rt @[A-Za-z0-9_]+:', '', regex=True) 
  #Removes mention just containing @word only
  df_tweets['text'] = df_tweets['text'].str.replace(r'@[A-Za-z0-9_]+', '', regex=True) 
  #Removing #tags 
  #df_tweets['text'] = df_tweets['text'].str.replace(r'#[A-Za-z0-9_]+', '', regex=True)  

  #Removing links
  df_tweets['text'] = df_tweets['text'].str.replace(r'http\S+', '', regex=True)
  df_tweets['text'] = df_tweets['text'].str.replace(r'www.\S+', '', regex=True) 

  #Removing punctuations and replacing with a single space
  df_tweets['text'] = df_tweets['text'].str.replace(r'[()!?]', ' ', regex=True)  
  df_tweets['text'] = df_tweets['text'].str.replace(r'\[.*?\]', ' ', regex=True)

  #Filtering non-alphanumeric characters
  df_tweets['text'] = df_tweets['text'].str.replace(r'[^a-z0-9]', ' ', regex=True) 

  #Removing Stoping words + keywords_to_hear
  stop = stopwords.words(['english', 'spanish', 'portuguese']) + ['l','pez', 'n', 'andr','p', 'si','est', 'c', 'qu']
  df_tweets['tweet_without_stopwords'] = df_tweets['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

#preview of the data before cleaning
df_tweets.head()

text_clean(df_tweets)
# tokenize the tweets
df_tweets['tokenized_sents'] = df_tweets.apply(lambda row: nltk.word_tokenize(row['tweet_without_stopwords']), axis=1)
df_tweets.head() #preview of the data after cleaning

"""### Visualizing/InfoGraphics the text column (Unigram)"""

# Create a list of lists containing words for each tweet
words_in_tweet = list(df_tweets['tokenized_sents'])
words_in_tweet[:2]

#Calculate word frequencies
# List of all words across tweets
all_words = list(itertools.chain(*words_in_tweet))

# Create counter
counts_words = collections.Counter(all_words)

counts_words.most_common(15)

# transform the list into a pandas dataframe
df_counts_words = pd.DataFrame(counts_words.most_common(15),
                             columns=['words', 'count'])

df_counts_words.head(10)

#A horizontal bar graph to visualize the most common words
fig, ax = plt.subplots(figsize=(10, 8))

# Plot horizontal bar graph
df_counts_words.sort_values(by='count').plot.barh(x='words',
                      y='count',
                      ax=ax,
                      color="green")

ax.set_title("Common Words Found in Tweets ",  fontsize=15, color= 'violet', fontweight='bold')
plt.savefig('count_unigram.png')
plt.show()

from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
wordcloud2 = WordCloud(background_color="white",
                       max_words=100,
                       height=3000, width=3000,
                       colormap='Set2',
                       collocations=False,
                       repeat=True).generate(' '.join(df_counts_words["words"]))
# Generate plot
plt.figure(figsize=(10,7), facecolor='k')

plt.tight_layout(pad=0)
plt.imshow(wordcloud2, interpolation='bilinear')
plt.axis("off")
plt.savefig('cloud_uni.png')
plt.show()



"""## Collection of Words â€“ Bigrams"""

#Create a list of tokenized_sents
tweets_words = list(df_tweets['tokenized_sents'])
tweets_words[2]

#Remove any empty lists
tweets_words_new = [x for x in tweets_words if x != []]
tweets_words_new[2]

# Create list of lists containing bigrams in tweets
terms_bigram = [list(bigrams(tweet)) for tweet in tweets_words_new]

# View bigrams for the first tweet
terms_bigram[2]

# Flatten list of bigrams in clean tweets
bigrams = list(itertools.chain(*terms_bigram))

# Create counter of words in clean bigrams
bigram_counts = collections.Counter(bigrams)

bigram_counts.most_common(20)

#Creating a dataframe of the most common bigrams
bigram_df = pd.DataFrame(bigram_counts.most_common(20),
                             columns=['bigram', 'count'])

bigram_df

