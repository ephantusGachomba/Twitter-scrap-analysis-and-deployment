{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        ":The objectives of this analysis is to analyse the data that has been collected and get meaningful insights from it."
      ],
      "metadata": {
        "id": "y-Yy82CdD_7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries"
      ],
      "metadata": {
        "id": "Y2WSgXPaErGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the libraries to be used\n",
        "#import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "import collections\n",
        "import re\n",
        "\n",
        "#import panel as pn\n",
        "#pn.extension('tabulator')\n",
        "#!pip install hvplot\n",
        "#import hvplot.pandas\n",
        "\n",
        "import tweepy as tw\n",
        "from textblob import TextBlob\n",
        "#!pip install pygal\n",
        "#import pygal\n",
        "!pip install vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer \n",
        "from nltk import bigrams\n",
        "from textblob import TextBlob\n",
        "import networkx # for creating networknodes\n",
        "import networkx as nx\n",
        "#from pandas.io.json import json_normalize\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "sns.set(font_scale=1.5)\n",
        "sns.set_style(\"whitegrid\")"
      ],
      "metadata": {
        "id": "qzunwCRARO-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading the Dataset and Getting Info about it\n"
      ],
      "metadata": {
        "id": "k7VH3_lL8KUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading the dataset\n",
        "df = pd.read_csv('dataset_twitter-scraper.csv')\n",
        "df.rename(columns={\"full_text\": \"text\"}, inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "EAqj3PEMJRDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we shall be working with text and created_at columns.\n",
        "df = df[[\"text\" ,\"created_at\"]]\n",
        "df.head(4)"
      ],
      "metadata": {
        "id": "YPhtDVJQJfDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the info of data\n",
        "df.info()"
      ],
      "metadata": {
        "id": "l3OgGae-JisV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing the duplicates\n",
        "df = df.drop_duplicates()"
      ],
      "metadata": {
        "id": "xlz1Cl3ZJmf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert the created_at column to datetime datatype\n",
        "df['created_at'] = df['created_at'].astype('datetime64[ns]')\n",
        "df.info()"
      ],
      "metadata": {
        "id": "SCm2Pjb4JnTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAwfMWe8wiGk"
      },
      "source": [
        "## The next step, it shows the top 5 text with more reactions:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# top 5 texts\n",
        "reactions = df.iloc[:,[0,1]].groupby(['text']).count()\n",
        "reactions.sort_values(by=['created_at'],ascending=False).iloc[0:5, :]\n"
      ],
      "metadata": {
        "id": "cH5VsOc1J_dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking the Date ranges and the peak hours"
      ],
      "metadata": {
        "id": "BsuoTXxI_tOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a column for hour\n",
        "df['hour'] = df['created_at'].dt.hour\n",
        "#Creating a column for days\n",
        "df['date'] = df['created_at'].dt.date\n",
        "#Creating a column for month\n",
        "df['month'] = df['created_at'].dt.month\n",
        "df.head()"
      ],
      "metadata": {
        "id": "e5F9YBAKKIKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the unique dates\n",
        "df['date'].value_counts()#Checking the unique dates"
      ],
      "metadata": {
        "id": "V25u8CX8KUsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# time series showing when the tweets for this analysis was created\n",
        "reactions = df.groupby(['date']).count()\n",
        "ax = reactions.text.plot(figsize=(15,6),ls='--',c='red')\n",
        "plt.ylabel('The Count of tweets collected')\n",
        "plt.title('A Trend on the counts of tweets and the dates created')\n",
        "ax.xaxis.grid(True)\n",
        "ax.yaxis.grid(True)"
      ],
      "metadata": {
        "id": "lgG0Ga1aKgfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reactions = df.groupby(['hour']).count().sort_values(by='created_at',ascending=0)\n",
        "reactions.head()"
      ],
      "metadata": {
        "id": "1qudngaQKlh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# time series plot for the most active hours for tweeting\n",
        "reactions = df.groupby(['hour']).count()\n",
        "ax = reactions.text.plot(figsize=(15,6),ls='--',c='green')\n",
        "plt.ylabel('The Count of tweets collected')\n",
        "plt.title('A Trend on the counts of tweets and the hours created')\n",
        "ax.xaxis.grid(True)\n",
        "ax.yaxis.grid(True)"
      ],
      "metadata": {
        "id": "t4nf9TEvKlYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b_p3L2-69oO"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a copy for the text column This will enable us work with the text column solely\n",
        "df_tweets = df[['text']].copy()\n",
        "df_tweets.tail(5)"
      ],
      "metadata": {
        "id": "-6q8ysGZKlR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping the duplicates\n",
        "df_tweets = df_tweets.drop_duplicates()"
      ],
      "metadata": {
        "id": "Nua6JSOGKlMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tweets.tail(5)\n"
      ],
      "metadata": {
        "id": "lttqTx-3K9YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDBhp9-NCXWu"
      },
      "source": [
        "## Text Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcrK1Bqt9uut"
      },
      "outputs": [],
      "source": [
        "#A Function for cleaning the file (The text column in it)\n",
        "def text_clean(df_tweets):\n",
        "  #Lowercasing all the letters\n",
        "  df_tweets['text'] = df_tweets['text'].str.lower() \n",
        "\n",
        "  #Removes mentions containing rt word\n",
        "  df_tweets['text'] = df_tweets['text'].str.replace(r'rt @[A-Za-z0-9_]+:', '', regex=True) \n",
        "  #Removes mention just containing @word only\n",
        "  df_tweets['text'] = df_tweets['text'].str.replace(r'@[A-Za-z0-9_]+', '', regex=True) \n",
        "  #Removing #tags \n",
        "  #df_tweets['text'] = df_tweets['text'].str.replace(r'#[A-Za-z0-9_]+', '', regex=True)  \n",
        "\n",
        "  #Removing links\n",
        "  df_tweets['text'] = df_tweets['text'].str.replace(r'http\\S+', '', regex=True)\n",
        "  df_tweets['text'] = df_tweets['text'].str.replace(r'www.\\S+', '', regex=True) \n",
        "\n",
        "  #Removing punctuations and replacing with a single space\n",
        "  df_tweets['text'] = df_tweets['text'].str.replace(r'[()!?]', ' ', regex=True)  \n",
        "  df_tweets['text'] = df_tweets['text'].str.replace(r'\\[.*?\\]', ' ', regex=True)\n",
        "\n",
        "  #Filtering non-alphanumeric characters\n",
        "  df_tweets['text'] = df_tweets['text'].str.replace(r'[^a-z0-9]', ' ', regex=True) \n",
        "\n",
        "  #Removing Stoping words + keywords_to_hear\n",
        "  stop = stopwords.words('english') + ['n', '2','5', '000'] \n",
        "  df_tweets['tweet_without_stopwords'] = df_tweets['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preview of the data before cleaning\n",
        "df_tweets.head()"
      ],
      "metadata": {
        "id": "f9_Pydv_Mv68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_clean(df_tweets)\n",
        "# tokenize the tweets\n",
        "df_tweets['tokenized_sents'] = df_tweets.apply(lambda row: nltk.word_tokenize(row['tweet_without_stopwords']), axis=1)\n",
        "df_tweets.head() #preview of the data after cleaning"
      ],
      "metadata": {
        "id": "zk3ShwOIMzUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLF0xRYw6zKO"
      },
      "source": [
        "### Visualizing/InfoGraphics the text column (Unigram)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of lists containing words for each tweet\n",
        "words_in_tweet = list(df_tweets['tokenized_sents'])\n",
        "words_in_tweet[:2]"
      ],
      "metadata": {
        "id": "qxoY4P6CM7R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate word frequencies\n",
        "# List of all words across tweets\n",
        "all_words = list(itertools.chain(*words_in_tweet))\n",
        "\n",
        "# Create counter\n",
        "counts_words = collections.Counter(all_words)\n",
        "\n",
        "counts_words.most_common(15)"
      ],
      "metadata": {
        "id": "u5QJswBjM_SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform the list into a pandas dataframe\n",
        "df_counts_words = pd.DataFrame(counts_words.most_common(15),\n",
        "                             columns=['words', 'count'])\n",
        "\n",
        "df_counts_words.head(10)"
      ],
      "metadata": {
        "id": "5xpSMC8mNCr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A horizontal bar graph to visualize the most common words\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "# Plot horizontal bar graph\n",
        "df_counts_words.sort_values(by='count').plot.barh(x='words',\n",
        "                      y='count',\n",
        "                      ax=ax,\n",
        "                      color=\"green\")\n",
        "\n",
        "ax.set_title(\"Common Words Found in Tweets \")\n",
        "plt.savefig('count_unigram.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R0qr1Rg0NGfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "wordcloud2 = WordCloud(background_color=\"black\",max_words=100,width=3000, height=2000,repeat=True).generate(' '.join(df_counts_words[\"words\"]))\n",
        "# Generate plot\n",
        "plt.tight_layout(pad=0)\n",
        "plt.figure(figsize=(10,7), facecolor='k')\n",
        "plt.imshow(wordcloud2)\n",
        "plt.axis(\"off\",interpolation=\"bilinear\")\n",
        "plt.savefig('cloud_uni.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IdL5Jd0sNKFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceIIrWVjqQBL"
      },
      "source": [
        "## Collection of Words – Bigrams"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a list of tokenized_sents\n",
        "tweets_words = list(df_tweets['tokenized_sents'])\n",
        "tweets_words[2]"
      ],
      "metadata": {
        "id": "K8kL2dKtNR2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove any empty lists\n",
        "tweets_words_new = [x for x in tweets_words if x != []]\n",
        "tweets_words_new[2]"
      ],
      "metadata": {
        "id": "Yq8FqSdSNUvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create list of lists containing bigrams in tweets\n",
        "terms_bigram = [list(bigrams(tweet)) for tweet in tweets_words_new]\n",
        "\n",
        "# View bigrams for the first tweet\n",
        "terms_bigram[2]"
      ],
      "metadata": {
        "id": "AJGR95weNX1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten list of bigrams in clean tweets\n",
        "bigrams = list(itertools.chain(*terms_bigram))\n",
        "\n",
        "# Create counter of words in clean bigrams\n",
        "bigram_counts = collections.Counter(bigrams)\n",
        "\n",
        "bigram_counts.most_common(20)"
      ],
      "metadata": {
        "id": "TVNHItmTNbD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a dataframe of the most common bigrams\n",
        "bigram_df = pd.DataFrame(bigram_counts.most_common(20),\n",
        "                             columns=['bigram', 'count'])\n",
        "\n",
        "bigram_df"
      ],
      "metadata": {
        "id": "CQSK5il3NfcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Networks of Bigrams"
      ],
      "metadata": {
        "id": "gzU2WLGosPFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dictionary of bigrams and their counts\n",
        "d = bigram_df.set_index('bigram').T.to_dict('records')\n",
        "\n",
        "# Create network plot \n",
        "G = nx.Graph()\n",
        "\n",
        "# Create connections between nodes\n",
        "for k, v in d[0].items():\n",
        "    G.add_edge(k[0], k[1], weight=(v * 10))"
      ],
      "metadata": {
        "id": "6SkEESnPNnq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(20, 15))\n",
        "\n",
        "pos = nx.spring_layout(G, k=2)\n",
        "\n",
        "# Plot networks\n",
        "nx.draw_networkx(G, pos,\n",
        "                 font_size=16,\n",
        "                 width=3,\n",
        "                 edge_color='red',\n",
        "                 node_color='black',\n",
        "                 with_labels = False,\n",
        "                 ax=ax)\n",
        "\n",
        "# Create offset labels\n",
        "for key, value in pos.items():\n",
        "    x, y = value[0]+.135, value[1]+.045\n",
        "    ax.text(x, y,\n",
        "            s=key,\n",
        "            bbox=dict(facecolor='aqua', alpha=0.25),\n",
        "            horizontalalignment='center', fontsize=17)\n",
        "plt.title('Visualize Networks of Bigrams')  \n",
        "plt.savefig('bigrams_network.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wTU9YCwKN3OA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Polarity"
      ],
      "metadata": {
        "id": "Iw7SqCb3O8zF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to get the subjectivity Subjectivity refers to an individual's feelings, opinions, or preferences.\n",
        "def getSubjectivity(text):\n",
        "  return TextBlob(text).sentiment.subjectivity\n",
        "\n",
        "#Create a function to get the polarity (Tells how positive or negative the text is)\n",
        "def getPolarity(text):\n",
        "  return TextBlob(text).sentiment.polarity\n",
        "\n",
        "#Create two new columns\n",
        "df['Subjectivity'] = df['text'].apply(getSubjectivity)\n",
        "df['Polarity'] = df['text'].apply(getPolarity)\n",
        "\n",
        "#show the new dataframe with columns\n",
        "df"
      ],
      "metadata": {
        "id": "TsTdayMtOTq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the WordCloud\n",
        "allwords  = ' '.join([txts for txts in df['text']])\n",
        "wordCloud = WordCloud(width = 500, height = 300, random_state = 21, max_font_size=119).generate(allwords)\n",
        "\n",
        "plt.imshow(wordCloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uveSDrBiOa4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create fxn to compute negative , neutral and positive analysis\n",
        "def getAnalysis(score):\n",
        "  if score < 0:\n",
        "    return 'Negative'\n",
        "  elif score == 0:\n",
        "    return 'Neutral'\n",
        "  else:\n",
        "    return 'Positive'\n",
        "\n",
        "df['Analysis'] = df['Polarity'].apply(getAnalysis)\n",
        "\n",
        "#Show the dataframe\n",
        "df"
      ],
      "metadata": {
        "id": "bNkV8r1POfP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sortedDF = df.sort_values(by='Polarity')\n",
        "sortedDF"
      ],
      "metadata": {
        "id": "daVGEpETOlJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the polarity and subjectivity\n",
        "plt.figure(figsize=(28,10))\n",
        "for i in range(0, 247): #The range is the number of rows in our dataset\n",
        "  plt.scatter(df['Polarity'][i], df['Subjectivity'][i], color='black')\n",
        "plt.title(\"Sentiment Analysis Distribution\")\n",
        "plt.xlabel('Polarity')\n",
        "plt.ylabel('Subjectivity')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c7wgGsDEOqLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the Percentage of positive tweets\n",
        "ptweets = df[df.Analysis == 'Positive']\n",
        "ptweets = ptweets['text']\n",
        "\n",
        "round((ptweets.shape[0] / df.shape[0]) * 100, 1)"
      ],
      "metadata": {
        "id": "1p8E4J6vOuXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the Percentage of negative tweets\n",
        "ntweets = df[df.Analysis == 'Negative']\n",
        "ntweets = ntweets['text']\n",
        "\n",
        "round((ntweets.shape[0] / df.shape[0]) * 100, 1)"
      ],
      "metadata": {
        "id": "xgzKMzTUOxb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Show the Value counts\n",
        "sns.countplot(x='Analysis', data=df)\n",
        "\n",
        "#plot and visualize the counts\n",
        "plt.title('Sentiment Analysis')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Counts')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ePO-dVXTO1LX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# polarity ( positive, negative , and neutral scores for each tweet)"
      ],
      "metadata": {
        "id": "ASosTVA0xmre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''using polarity_scores() we, \n",
        "will find all the positive, negative, and neutral scores for each tweet.'''\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "scores = []\n",
        "# Declare variables for scores\n",
        "compound_list = []\n",
        "positive_list = []\n",
        "negative_list = []\n",
        "neutral_list = []\n",
        "for i in range(df['text'].shape[0]):\n",
        "#print(analyser.polarity_scores(sentiments_pd['text'][i]))\n",
        "    compound = analyzer.polarity_scores(df['text'][i])[\"compound\"]\n",
        "    pos = analyzer.polarity_scores(df['text'][i])[\"pos\"]\n",
        "    neu = analyzer.polarity_scores(df['text'][i])[\"neu\"]\n",
        "    neg = analyzer.polarity_scores(df['text'][i])[\"neg\"]\n",
        "    \n",
        "    scores.append({\"Compound\": compound,\n",
        "                       \"Positive\": pos,\n",
        "                       \"Negative\": neg,\n",
        "                       \"Neutral\": neu\n",
        "                  })"
      ],
      "metadata": {
        "id": "134WlLeexr5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m-Jfgr8QPCWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting the scores dictionary containing the scores into the data frame, then join the sentiments_score data frame with the df data frame."
      ],
      "metadata": {
        "id": "xR6b5I85iLGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiments_score = pd.DataFrame.from_dict(scores)\n",
        "df = df.join(sentiments_score)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "OEh_Y_47QiZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the percentages of +ve, -ve and neutral\n",
        "#Calculating the percentages\n",
        "def percentage_polarity(part, whole_data):\n",
        "  percentage = 100 * float(part) / float(whole_data)\n",
        "  return round(percentage, 1)\n",
        "\n",
        "negative = 0\n",
        "positive = 0\n",
        "neutral = 0\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "  neg = row['Negative']\n",
        "  pos = row['Positive']\n",
        "  if neg > pos :\n",
        "    negative += 1\n",
        "    negative_list.append(df.text)\n",
        "  elif pos > neg :\n",
        "    positive += 1\n",
        "  elif pos == neg:\n",
        "    neutral += 1\n",
        "\n",
        "positive_percentage = percentage_polarity(positive, df.shape[0])\n",
        "negative_percentage = percentage_polarity(negative, df.shape[0])\n",
        "neutral_percentage = percentage_polarity(neutral, df.shape[0])\n",
        "\n",
        "print(f\"Negative : Counts {negative} Its Percentage = {negative_percentage}%\")\n",
        "print(f\"positive : Counts {positive}  Its Percentage = {positive_percentage}%\")\n",
        "print(f\"neutral : Counts {neutral}  Its Percentage = {neutral_percentage}%\")"
      ],
      "metadata": {
        "id": "XSJvxiKaQuot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating PieCart\n",
        "labels = ['Positive ['+str(positive_percentage)+'%]' , 'Neutral ['+str(neutral_percentage)+'%]','Negative ['+str(negative_percentage)+'%]']\n",
        "sizes = [positive_percentage, neutral_percentage, negative_percentage]\n",
        "colors = ['black', 'green','red']\n",
        "my_circle=plt.Circle( (0,0), 0.5, color='white')\n",
        "patches, texts = plt.pie(sizes,colors=colors, startangle=90)\n",
        "p=plt.gcf()\n",
        "p.gca().add_artist(my_circle)\n",
        "plt.style.use('default')\n",
        "plt.legend(labels)\n",
        "plt.title(\"Sentiment Analysis Result \" )\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "68grKhODQy9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "19NmQo6KQ3oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# plotting wordcloud for positive, neutral and negative"
      ],
      "metadata": {
        "id": "2WRUfHIuze4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "def word_cloud(wd_list):\n",
        "    stopwords = set(STOPWORDS)\n",
        "    all_words = ' '.join([text for text in wd_list])\n",
        "    wordcloud = WordCloud(\n",
        "        background_color='black',\n",
        "        stopwords=stopwords,\n",
        "        width=1600,\n",
        "        height=800,\n",
        "        random_state=1,\n",
        "        colormap='jet',\n",
        "        max_words=80,\n",
        "        max_font_size=200).generate(all_words)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\");\n",
        "word_cloud(df['text'])"
      ],
      "metadata": {
        "id": "o-VHzlBEQ8fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Negative sentiment word cloud\n",
        "word_cloud(df['text'][df['Positive'] < df['Negative']])"
      ],
      "metadata": {
        "id": "y96Bf53aRAyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Positive sentiment word cloud\n",
        "word_cloud(df['text'][df['Positive'] > df['Negative']])"
      ],
      "metadata": {
        "id": "inf4SfONREAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Neutral cloud\n",
        "word_cloud(df['text'][df['Positive'] == df['Negative']])\n"
      ],
      "metadata": {
        "id": "zoJDH5vZRHcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Wordcloud is the informative visual representation of text datasets, highlighting the most popular and trending keywords in text datasets based on the frequency of occurrence and importance."
      ],
      "metadata": {
        "id": "0YRYPiJfjFSB"
      }
    }
  ]
}